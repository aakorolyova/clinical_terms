{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9e3eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "from utils import get_word_embeddings, get_bert_vector, get_avg_similarity, get_bert_vector_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2990003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term1</th>\n",
       "      <th>Term2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mittelschmerz</td>\n",
       "      <td>actonel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>incontinence</td>\n",
       "      <td>lasix</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hyperemesis</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>myositis</td>\n",
       "      <td>lipitor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>plavix</td>\n",
       "      <td>ambivalent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Term1       Term2  label\n",
       "0  mittelschmerz     actonel      1\n",
       "1   incontinence       lasix      1\n",
       "2    hyperemesis        free      0\n",
       "3       myositis     lipitor      1\n",
       "4         plavix  ambivalent      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'data\\MedicalConcepts_augmented.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77598424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term1</th>\n",
       "      <th>Term2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mittelschmerz</td>\n",
       "      <td>actonel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>incontinence</td>\n",
       "      <td>lasix</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>myositis</td>\n",
       "      <td>lipitor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dysuria</td>\n",
       "      <td>cipro</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>enalapril</td>\n",
       "      <td>lisinopril</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Term1       Term2  label\n",
       "0  mittelschmerz     actonel      1\n",
       "1   incontinence       lasix      1\n",
       "3       myositis     lipitor      1\n",
       "8        dysuria       cipro      1\n",
       "9      enalapril  lisinopril      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_terms = df[df.label==1]\n",
    "related_terms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73a8bc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term1</th>\n",
       "      <th>Term2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hyperemesis</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>plavix</td>\n",
       "      <td>ambivalent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>drooling</td>\n",
       "      <td>changes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>otitis</td>\n",
       "      <td>clamping</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>clubbing</td>\n",
       "      <td>stilts</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Term1       Term2  label\n",
       "2  hyperemesis        free      0\n",
       "4       plavix  ambivalent      0\n",
       "5     drooling     changes      0\n",
       "6       otitis    clamping      0\n",
       "7     clubbing      stilts      0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unrelated_terms = df[df.label==0]\n",
    "unrelated_terms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95678397",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_terms = sorted(set(df.Term1.tolist() + df.Term2.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f1b5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = get_word_embeddings(\n",
    "      r'C:\\Users\\Anna\\Files\\SemLex\\embeddings\\glove\\glove-embeddings\\glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0848fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embeddings = get_word_embeddings(\n",
    "      r'C:\\Users\\Anna\\Files\\SemLex\\embeddings\\fasttext_aligned_cleaned\\wiki.en.align.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f0cb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at allenai/scibert_scivocab_uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_scibert = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model_scibert = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc4409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/config.json from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\f048b8136bae2b3abe91e9e82949295fb205887c84db3be2775e1cdb0ecfeeb9.d7812d36d3371e4d43299a0c4a938622c5251db0efa17a5d4d9b57037fcec823\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-v1.1\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/vocab.txt from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\cda52d3a8283b321708097045e27f11cd70bbf3ad8cdefa2c0a56f187855f5d5.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/special_tokens_map.json from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\118da8438a7854000cfcf052566f83ae4f4159ac25796e49e16c3b18746041b4.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/tokenizer_config.json from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\2fb6c5805404829e9c10c33b38ae59ae3011225799f3177f769a06a7411fa46c.25d8d06fb0679146a3ed2a3463e3585380bff882fe6e1ebc497196e40dbbd7fa\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/config.json from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\f048b8136bae2b3abe91e9e82949295fb205887c84db3be2775e1cdb0ecfeeb9.d7812d36d3371e4d43299a0c4a938622c5251db0efa17a5d4d9b57037fcec823\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-v1.1\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/config.json from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\f048b8136bae2b3abe91e9e82949295fb205887c84db3be2775e1cdb0ecfeeb9.d7812d36d3371e4d43299a0c4a938622c5251db0efa17a5d4d9b57037fcec823\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-v1.1\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/config.json from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\f048b8136bae2b3abe91e9e82949295fb205887c84db3be2775e1cdb0ecfeeb9.d7812d36d3371e4d43299a0c4a938622c5251db0efa17a5d4d9b57037fcec823\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-v1.1\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/pytorch_model.bin from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\65231a5792b14eb81b9a6bdccccfffda18575eb3bafbb730c9fa4235e56c3c17.74cc2087932cb523a583bd5e65732ee1aaade59dfc0b62f88101de7567d92e42\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at dmis-lab/biobert-v1.1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_biobert = AutoTokenizer.from_pretrained('dmis-lab/biobert-v1.1')\n",
    "model_biobert = AutoModel.from_pretrained('dmis-lab/biobert-v1.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b677c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Anna/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file models\\bert_mlm\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file models\\bert_mlm\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at models\\bert_mlm were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at models\\bert_mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file models\\bert_fine_tuned\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BERTClass\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file models\\bert_fine_tuned\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at models\\bert_fine_tuned were not used when initializing BertModel: ['l1.encoder.layer.11.attention.output.dense.bias', 'l1.encoder.layer.11.output.LayerNorm.bias', 'l1.encoder.layer.3.attention.self.value.weight', 'l1.encoder.layer.9.output.LayerNorm.weight', 'l1.encoder.layer.5.output.dense.weight', 'l1.encoder.layer.8.attention.self.value.weight', 'l3.bias', 'l1.encoder.layer.9.attention.self.key.bias', 'l1.encoder.layer.6.attention.self.query.bias', 'l1.encoder.layer.2.attention.output.dense.weight', 'l1.encoder.layer.0.output.LayerNorm.bias', 'l1.encoder.layer.3.output.dense.bias', 'l1.encoder.layer.9.output.dense.bias', 'l1.encoder.layer.1.attention.self.key.weight', 'l1.encoder.layer.0.attention.output.dense.bias', 'l1.encoder.layer.5.attention.self.value.bias', 'l1.encoder.layer.10.attention.self.value.weight', 'l1.encoder.layer.5.attention.self.query.bias', 'l1.encoder.layer.6.attention.self.value.bias', 'l1.encoder.layer.2.output.LayerNorm.weight', 'l1.encoder.layer.10.attention.self.query.bias', 'l1.encoder.layer.11.attention.output.LayerNorm.weight', 'l1.encoder.layer.4.attention.output.dense.bias', 'l1.encoder.layer.10.attention.output.LayerNorm.weight', 'l1.encoder.layer.6.attention.output.dense.weight', 'l1.encoder.layer.6.output.dense.bias', 'l1.encoder.layer.6.attention.output.LayerNorm.bias', 'l1.encoder.layer.10.attention.self.key.weight', 'l1.embeddings.LayerNorm.weight', 'l1.encoder.layer.7.attention.output.LayerNorm.weight', 'l1.encoder.layer.8.intermediate.dense.bias', 'l1.encoder.layer.9.attention.output.dense.weight', 'l1.encoder.layer.11.intermediate.dense.weight', 'l1.pooler.dense.bias', 'l1.encoder.layer.2.attention.output.dense.bias', 'l1.encoder.layer.0.attention.self.query.bias', 'l1.encoder.layer.10.output.LayerNorm.bias', 'l1.encoder.layer.5.attention.output.LayerNorm.weight', 'l1.encoder.layer.11.attention.self.key.weight', 'l1.encoder.layer.9.attention.self.query.bias', 'l1.encoder.layer.0.attention.output.dense.weight', 'l1.encoder.layer.4.attention.output.dense.weight', 'l1.encoder.layer.10.attention.self.value.bias', 'l1.encoder.layer.7.output.dense.bias', 'l1.encoder.layer.11.attention.self.value.weight', 'l1.encoder.layer.5.output.LayerNorm.bias', 'l1.encoder.layer.1.attention.self.query.bias', 'l1.encoder.layer.0.output.dense.bias', 'l1.encoder.layer.0.attention.output.LayerNorm.weight', 'l1.encoder.layer.9.attention.self.query.weight', 'l1.encoder.layer.4.output.LayerNorm.weight', 'l1.encoder.layer.3.attention.output.LayerNorm.bias', 'l1.embeddings.position_embeddings.weight', 'l1.encoder.layer.6.output.LayerNorm.bias', 'l1.encoder.layer.5.attention.output.dense.bias', 'l1.encoder.layer.6.output.dense.weight', 'l1.encoder.layer.9.output.dense.weight', 'l1.encoder.layer.3.attention.output.dense.weight', 'l1.encoder.layer.11.output.LayerNorm.weight', 'l1.encoder.layer.0.intermediate.dense.bias', 'l1.encoder.layer.9.attention.self.value.weight', 'l1.encoder.layer.9.output.LayerNorm.bias', 'l1.encoder.layer.1.attention.self.key.bias', 'l1.encoder.layer.5.attention.output.LayerNorm.bias', 'l1.encoder.layer.9.attention.self.value.bias', 'l1.encoder.layer.1.attention.output.LayerNorm.bias', 'l1.encoder.layer.2.attention.self.query.weight', 'l1.encoder.layer.2.attention.output.LayerNorm.weight', 'l1.encoder.layer.4.attention.output.LayerNorm.bias', 'l1.encoder.layer.0.attention.self.query.weight', 'l1.encoder.layer.8.attention.output.LayerNorm.weight', 'l1.encoder.layer.10.attention.output.dense.bias', 'l1.encoder.layer.8.attention.output.LayerNorm.bias', 'l1.encoder.layer.0.intermediate.dense.weight', 'l1.encoder.layer.6.attention.self.value.weight', 'l1.encoder.layer.0.attention.output.LayerNorm.bias', 'l1.encoder.layer.9.intermediate.dense.weight', 'l1.encoder.layer.10.attention.output.dense.weight', 'l1.encoder.layer.6.output.LayerNorm.weight', 'l1.encoder.layer.11.output.dense.weight', 'l1.encoder.layer.8.attention.self.key.weight', 'l1.encoder.layer.9.attention.output.dense.bias', 'l1.encoder.layer.7.attention.self.key.weight', 'l1.encoder.layer.3.attention.self.query.weight', 'l1.encoder.layer.10.attention.self.key.bias', 'l1.encoder.layer.10.output.dense.bias', 'l1.encoder.layer.7.attention.self.query.bias', 'l1.encoder.layer.2.intermediate.dense.weight', 'l1.encoder.layer.7.attention.output.LayerNorm.bias', 'l1.encoder.layer.7.intermediate.dense.bias', 'l1.encoder.layer.8.attention.self.value.bias', 'l1.encoder.layer.8.attention.output.dense.weight', 'l1.encoder.layer.2.attention.self.query.bias', 'l1.encoder.layer.4.attention.self.key.bias', 'l1.encoder.layer.6.intermediate.dense.weight', 'l1.encoder.layer.11.attention.self.value.bias', 'l1.encoder.layer.7.attention.output.dense.bias', 'l1.encoder.layer.1.output.LayerNorm.weight', 'l1.embeddings.LayerNorm.bias', 'l1.encoder.layer.9.attention.self.key.weight', 'l1.encoder.layer.8.output.dense.bias', 'l1.embeddings.word_embeddings.weight', 'l1.encoder.layer.3.attention.output.dense.bias', 'l1.encoder.layer.10.intermediate.dense.bias', 'l1.encoder.layer.10.output.LayerNorm.weight', 'l1.encoder.layer.1.output.LayerNorm.bias', 'l1.encoder.layer.2.attention.self.value.weight', 'l1.encoder.layer.3.attention.self.key.bias', 'l1.encoder.layer.0.attention.self.key.weight', 'l3.weight', 'l1.encoder.layer.0.attention.self.key.bias', 'l1.encoder.layer.11.output.dense.bias', 'l1.encoder.layer.10.attention.output.LayerNorm.bias', 'l1.encoder.layer.6.attention.output.dense.bias', 'l1.encoder.layer.6.attention.output.LayerNorm.weight', 'l1.encoder.layer.4.output.dense.weight', 'l1.encoder.layer.1.output.dense.weight', 'l1.encoder.layer.4.intermediate.dense.bias', 'l1.encoder.layer.9.attention.output.LayerNorm.weight', 'l1.encoder.layer.7.output.LayerNorm.weight', 'l1.encoder.layer.10.output.dense.weight', 'l1.encoder.layer.2.attention.output.LayerNorm.bias', 'l1.encoder.layer.1.intermediate.dense.bias', 'l1.encoder.layer.5.intermediate.dense.bias', 'l1.encoder.layer.3.attention.self.key.weight', 'l1.encoder.layer.4.attention.self.value.bias', 'l1.encoder.layer.10.intermediate.dense.weight', 'l1.encoder.layer.11.attention.self.key.bias', 'l1.encoder.layer.4.intermediate.dense.weight', 'l1.encoder.layer.3.intermediate.dense.bias', 'l1.encoder.layer.7.attention.self.query.weight', 'l1.embeddings.token_type_embeddings.weight', 'l1.encoder.layer.4.attention.self.query.bias', 'l1.encoder.layer.8.output.dense.weight', 'l1.encoder.layer.7.output.LayerNorm.bias', 'l1.encoder.layer.2.output.dense.weight', 'l1.encoder.layer.4.attention.output.LayerNorm.weight', 'l1.encoder.layer.5.attention.output.dense.weight', 'l1.encoder.layer.8.attention.self.query.bias', 'l1.encoder.layer.1.intermediate.dense.weight', 'l1.encoder.layer.8.output.LayerNorm.bias', 'l1.encoder.layer.4.output.dense.bias', 'l1.encoder.layer.9.intermediate.dense.bias', 'l1.encoder.layer.1.attention.self.value.weight', 'l1.encoder.layer.2.intermediate.dense.bias', 'l1.encoder.layer.3.attention.self.query.bias', 'l1.encoder.layer.2.output.dense.bias', 'l1.encoder.layer.5.output.LayerNorm.weight', 'l1.encoder.layer.1.attention.self.query.weight', 'l1.encoder.layer.1.attention.output.LayerNorm.weight', 'l1.encoder.layer.11.attention.output.LayerNorm.bias', 'l1.encoder.layer.0.attention.self.value.weight', 'l1.encoder.layer.11.attention.self.query.bias', 'l1.encoder.layer.5.attention.self.key.weight', 'l1.encoder.layer.1.attention.output.dense.weight', 'l1.encoder.layer.2.attention.self.key.weight', 'l1.encoder.layer.4.attention.self.query.weight', 'l1.encoder.layer.8.attention.output.dense.bias', 'l1.encoder.layer.3.output.dense.weight', 'l1.encoder.layer.7.attention.self.value.bias', 'l1.encoder.layer.0.attention.self.value.bias', 'l1.encoder.layer.0.output.dense.weight', 'l1.encoder.layer.1.attention.output.dense.bias', 'l1.encoder.layer.10.attention.self.query.weight', 'l1.encoder.layer.8.intermediate.dense.weight', 'l1.encoder.layer.5.intermediate.dense.weight', 'l1.encoder.layer.1.attention.self.value.bias', 'l1.encoder.layer.5.output.dense.bias', 'l1.encoder.layer.5.attention.self.query.weight', 'l1.encoder.layer.1.output.dense.bias', 'l1.encoder.layer.11.attention.output.dense.weight', 'l1.encoder.layer.11.attention.self.query.weight', 'l1.encoder.layer.8.attention.self.query.weight', 'l1.encoder.layer.9.attention.output.LayerNorm.bias', 'l1.encoder.layer.8.output.LayerNorm.weight', 'l1.encoder.layer.6.attention.self.key.bias', 'l1.encoder.layer.6.attention.self.key.weight', 'l1.encoder.layer.8.attention.self.key.bias', 'l1.encoder.layer.2.attention.self.key.bias', 'l1.encoder.layer.4.attention.self.key.weight', 'l1.encoder.layer.6.intermediate.dense.bias', 'l1.encoder.layer.3.output.LayerNorm.bias', 'l1.encoder.layer.3.output.LayerNorm.weight', 'l1.embeddings.position_ids', 'l1.pooler.dense.weight', 'l1.encoder.layer.7.attention.self.key.bias', 'l1.encoder.layer.4.output.LayerNorm.bias', 'l1.encoder.layer.2.attention.self.value.bias', 'l1.encoder.layer.3.attention.output.LayerNorm.weight', 'l1.encoder.layer.0.output.LayerNorm.weight', 'l1.encoder.layer.7.attention.output.dense.weight', 'l1.encoder.layer.3.intermediate.dense.weight', 'l1.encoder.layer.6.attention.self.query.weight', 'l1.encoder.layer.3.attention.self.value.bias', 'l1.encoder.layer.2.output.LayerNorm.bias', 'l1.encoder.layer.4.attention.self.value.weight', 'l1.encoder.layer.7.attention.self.value.weight', 'l1.encoder.layer.7.output.dense.weight', 'l1.encoder.layer.5.attention.self.key.bias', 'l1.encoder.layer.5.attention.self.value.weight', 'l1.encoder.layer.11.intermediate.dense.bias', 'l1.encoder.layer.7.intermediate.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at models\\bert_fine_tuned and are newly initialized: ['encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'pooler.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'embeddings.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig()\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "model_bert_mlm = BertModel.from_pretrained(r'models\\bert_mlm')\n",
    "model_bert_fine_tuned = BertModel.from_pretrained(r'models\\bert_fine_tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "342bca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_scibert = get_bert_vector_dataset(all_terms, model_scibert, tokenizer_scibert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "750ba013",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_biobert = get_bert_vector_dataset(all_terms, model_biobert, tokenizer_biobert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "add4d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_bert = get_bert_vector_dataset(all_terms, model_bert, tokenizer_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a672620",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_bert_mlm = get_bert_vector_dataset(all_terms, model_bert_mlm, tokenizer_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d588db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_bert_fine_tuned = get_bert_vector_dataset(all_terms, model_bert_fine_tuned, tokenizer_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7233c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_method = {'glove': (glove_embeddings, 'non_contextual'),\n",
    "                 'fasttext': (fasttext_embeddings, 'non_contextual'),\n",
    "                 'scibert': (embeddings_scibert, 'bert'),\n",
    "                 'biobert': (embeddings_biobert, 'bert'),\n",
    "                 'bert': (embeddings_bert, 'bert'),\n",
    "                 'bert_mlm': (embeddings_bert_mlm, 'bert'),\n",
    "                 'bert_fine_tuned': (embeddings_bert_fine_tuned, 'bert')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83499b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove\n",
      "Number of missing terms: 283\n",
      "avg_sim_related: [[0.30563128]]\n",
      "avg_sim_unrelated: [[0.05330584]]\n",
      "fasttext\n",
      "Number of missing terms: 104\n",
      "avg_sim_related: [[0.47708768]]\n",
      "avg_sim_unrelated: [[0.26278204]]\n",
      "scibert\n",
      "Number of missing terms: 0\n",
      "avg_sim_related: [[0.8408807]]\n",
      "avg_sim_unrelated: [[0.7975625]]\n",
      "biobert\n",
      "Number of missing terms: 0\n",
      "avg_sim_related: [[0.95431215]]\n",
      "avg_sim_unrelated: [[0.93287104]]\n",
      "bert\n",
      "Number of missing terms: 0\n",
      "avg_sim_related: [[0.88583654]]\n",
      "avg_sim_unrelated: [[0.8622656]]\n",
      "bert_mlm\n",
      "Number of missing terms: 0\n",
      "avg_sim_related: [[0.97700816]]\n",
      "avg_sim_unrelated: [[0.971721]]\n",
      "bert_fine_tuned\n",
      "Number of missing terms: 0\n",
      "avg_sim_related: [[0.93542624]]\n",
      "avg_sim_unrelated: [[0.92965233]]\n"
     ]
    }
   ],
   "source": [
    "for name, (embeddings, embedding_type) in name_to_method.items():\n",
    "    print(name)\n",
    "    print('Number of missing terms:', len([term for term in all_terms if term not in embeddings.keys()]))\n",
    "    avg_sim_related = get_avg_similarity(related_terms.Term1.tolist(), related_terms.Term2.tolist(), embeddings, embedding_type)\n",
    "    avg_sim_unrelated = get_avg_similarity(unrelated_terms.Term1.tolist(), unrelated_terms.Term2.tolist(), embeddings, embedding_type)\n",
    "    avg_sim_unrelated_non_med = get_avg_similarity(unrelated_terms.Term1.tolist(), unrelated_terms.Term2.tolist(), embeddings, embedding_type)\n",
    "    print('avg_sim_related:', avg_sim_related)\n",
    "    print('avg_sim_unrelated:', avg_sim_unrelated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e87ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
